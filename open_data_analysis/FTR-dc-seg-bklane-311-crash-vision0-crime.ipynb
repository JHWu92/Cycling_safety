{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from shapely.geometry import Point\n",
    "from src.geom_helper import pts2segs_by_chunk\n",
    "from src.constants import (index_pt, index_seg, epsg_dc, fn_segments_dc, dir_data, \n",
    "                           fn_311_dc, fn_crash_dc, fns_crime_dc, fn_vision0_dc,\n",
    "                          fn_feature_crash_dc, fn_feature_311_dc, fn_feature_vision0_dc, fn_feature_crime_dc,\n",
    "                          fn_oepndc_bk_dc, fn_feature_seg_attr_dc, fn_feature_oepndc_bk_dc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_dc = gp.read_file(dir_data + fn_segments_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segment attributes features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seg_dc.DIRECTIONALITY = seg_dc.DIRECTIONALITY.apply(lambda x: 'Bi-direction' if x==2 else 'one-way')\n",
    "ftr_segs_col = ['DIRECTIONALITY', 'STREETTYPE', 'SHAPE_Length', 'SEGMENTTYPE']\n",
    "ftr_segs = seg_dc[ftr_segs_col]\n",
    "ftr_segs.index.name = index_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftr_segs.to_csv(dir_data+fn_feature_seg_attr_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bake lanes features in opendc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bk = pd.read_csv(dir_data + fn_oepndc_bk_dc)\n",
    "\n",
    "dum = pd.get_dummies(bk.FACILITY)\n",
    "bk = bk[['STREETSEGID']]\n",
    "bk['bkdc_total'] = 1\n",
    "bk = bk.merge(dum, left_index=True, right_index=True)\n",
    "\n",
    "ftr_bk = bk.groupby('STREETSEGID').sum().reset_index()\n",
    "\n",
    "slice_str = seg_dc[['STREETSEGID']].reset_index()\n",
    "slice_str.columns=['index_seg', 'STREETSEGID']\n",
    "ftr_bk = ftr_bk.merge(slice_str)\n",
    "ftr_bk.set_index('index_seg', inplace=True)\n",
    "ftr_bk.drop('STREETSEGID', axis=1).to_csv(dir_data + fn_feature_oepndc_bk_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 311 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (4,15,23,26,32,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "csr311 = pd.read_csv(dir_data + fn_311_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match 311 to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 738322 # chunks: 7.38322\n",
      "matching chunk: 0 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda2\\lib\\site-packages\\numpy\\lib\\function_base.py:2287: RuntimeWarning: invalid value encountered in find_intersects (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching chunk: 100000 200000\n",
      "matching chunk: 200000 300000\n",
      "matching chunk: 300000 400000\n",
      "matching chunk: 400000 500000\n",
      "matching chunk: 500000 600000\n",
      "matching chunk: 600000 700000\n",
      "matching chunk: 700000 738322\n"
     ]
    }
   ],
   "source": [
    "pts = csr311.apply(lambda x: Point(x.LONGITUDE, x.LATITUDE), axis=1)\n",
    "gpdf = gp.GeoDataFrame(pts, columns=['geometry'])\n",
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(gpdf, seg_dc, epsg_dc)\n",
    "pts_has_ln.columns = [index_pt, index_seg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean 311 data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((738322, 52), 733930)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csr311.shape, pts_has_ln.index_pt.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean csr311\n",
    "date = pd.to_datetime(csr311.INITIATEDDATE)\n",
    "csr311['MONTH'] = date.dt.month\n",
    "csr311['YEAR'] = date.dt.year\n",
    "\n",
    "# get dummies for types\n",
    "dummies = pd.get_dummies(csr311.DESCRIPTION, prefix='311')\n",
    "\n",
    "csr311['311_total'] = 1\n",
    "csr311['311_total_not_parking_meter'] = csr311.DESCRIPTION=='PARKING METER REQUES'\n",
    "csr311 = csr311[['MONTH','YEAR', '311_total', '311_total_not_parking_meter']]\n",
    "\n",
    "csr311 = csr311.merge(dummies, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_vision0 = pts_has_ln.merge(csr311, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_vision0.drop('index_pt', inplace=True, axis=1)\n",
    "print 'to group'\n",
    "ftr_311 = segs_with_vision0.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_311.to_csv(dir_data + fn_feature_311_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (4,5,6,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "crashes = pd.read_csv(dir_data+ fn_crash_dc)\n",
    "# clean crashes\n",
    "date = pd.to_datetime(crashes.SOURCEADDTIME)\n",
    "crashes['MONTH'] = date.dt.month\n",
    "crashes['YEAR'] = date.dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match crashes to segments:\n",
    "1. apply pts2seg\n",
    "2. for those(~70K) without coordinates or no matches, use STREETSEGID column in crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 124278 # chunks: 1.24278\n",
      "matching chunk: 0 100000\n",
      "matching chunk: 100000 124278\n"
     ]
    }
   ],
   "source": [
    "# match with segs\n",
    "no_xy = (crashes.X.isnull())|(crashes.Y.isnull())\n",
    "crashes_with_xy = crashes[~no_xy]\n",
    "pts = crashes_with_xy.apply(lambda x: Point(x.X, x.Y), axis=1)\n",
    "gpdf = gp.GeoDataFrame(pts, columns=['geometry'], index=crashes_with_xy.index)\n",
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(gpdf, seg_dc, epsg_dc)\n",
    "# raname columns\n",
    "pts_has_ln.columns = [index_pt, index_seg]\n",
    "\n",
    "# get STREETSEGID FOR crashes with matches\n",
    "crash_no_ln = crashes[~crashes.index.isin(set(pts_has_ln.index_pt))]\n",
    "crash_using_segid = crash_no_ln.loc[~crash_no_ln.STREETSEGID.isnull(), 'STREETSEGID'].reset_index()\n",
    "# get index_seg\n",
    "crash_using_segid = crash_using_segid.merge(seg_dc[['STREETSEGID']].reset_index(), left_on=\"STREETSEGID\", right_on=\"STREETSEGID\").drop('STREETSEGID', axis=1)\n",
    "# rename columns\n",
    "crash_using_segid.columns=[index_pt, index_seg]\n",
    "\n",
    "# append them\n",
    "pts_segs_index = pts_has_ln.append(crash_using_segid,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean crashes data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get dummies for types\n",
    "CRASHEVENTTYPES_dummies = crashes.CRASHEVENTTYPES.apply(lambda x: {i.strip():1 for i in x.split(',')}).apply(pd.Series)\n",
    "CRASHEVENTTYPES_dummies.columns = 'crash_evt_' + CRASHEVENTTYPES_dummies.columns\n",
    "\n",
    "FIRSTHARMFULEVENTSPECIFICS_dummies = pd.get_dummies(crashes.FIRSTHARMFULEVENTSPECIFICS)\n",
    "FIRSTHARMFULEVENTSPECIFICS_dummies.columns = 'crash_1stharm_' + FIRSTHARMFULEVENTSPECIFICS_dummies.columns\n",
    "FIRSTHARMFULEVENTSPECIFICS_dummies.replace(0, np.nan, inplace=True)\n",
    "\n",
    "crashes = crashes[['MONTH','YEAR']]\n",
    "crashes['crash_total'] = 1\n",
    "crashes = crashes.merge(CRASHEVENTTYPES_dummies, left_index=True, right_index=True)\n",
    "crashes = crashes.merge(FIRSTHARMFULEVENTSPECIFICS_dummies, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pts_segs_index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-42a0b1a3cf00>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msegs_with_crashes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpts_segs_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcrashes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft_on\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_pt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0msegs_with_crashes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'index_pt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mftr_crashes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msegs_with_crashes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'index_seg'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'YEAR'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'MONTH'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0magg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'sum'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mftr_crashes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir_data\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfn_feature_crash_dc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pts_segs_index' is not defined"
     ]
    }
   ],
   "source": [
    "segs_with_crashes = pts_segs_index.merge(crashes, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_crashes.drop('index_pt', inplace=True, axis=1)\n",
    "ftr_crashes = segs_with_crashes.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_crashes.to_csv(dir_data + fn_feature_crash_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vision zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vision0 = pd.read_csv(dir_data+ fn_vision0_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match v0 to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts2segs ing...\n"
     ]
    }
   ],
   "source": [
    "pts = vision0.apply(lambda x: Point(x.X, x.Y), axis=1)\n",
    "gpdf = gp.GeoDataFrame(pts, columns=['geometry'])\n",
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(gpdf, seg_dc, epsg_dc)\n",
    "pts_has_ln.columns = [index_pt, index_seg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean vision zero data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean crashes\n",
    "date = pd.to_datetime(vision0.REQUESTDATE)\n",
    "vision0['MONTH'] = date.dt.month\n",
    "vision0['YEAR'] = date.dt.year\n",
    "\n",
    "# get dummies for types\n",
    "USERTYPE_dummies = pd.get_dummies(vision0.USERTYPE)\n",
    "USERTYPE_dummies.columns = 'v0ur_'+USERTYPE_dummies.columns\n",
    "\n",
    "REQUESTTYPE_dummies = pd.get_dummies(vision0.REQUESTTYPE)\n",
    "REQUESTTYPE_dummies.columns = 'v0rq_' + REQUESTTYPE_dummies.columns\n",
    "\n",
    "vision0 = vision0[['MONTH','YEAR']]\n",
    "vision0['v0_total'] = 1\n",
    "vision0 = vision0.merge(USERTYPE_dummies, left_index=True, right_index=True)\n",
    "vision0 = vision0.merge(REQUESTTYPE_dummies, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_vision0 = pts_has_ln.merge(vision0, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_vision0.drop('index_pt', inplace=True, axis=1)\n",
    "ftr_v0 = segs_with_vision0.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_v0.to_csv(dir_data + fn_feature_vision0_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ys = [2014, 2015, 2016, 2017]\n",
    "crimes = [gp.read_file(dir_data + fns_crime_dc[y]) for y in ys]\n",
    "crime = pd.concat(crimes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match v0 to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 112912 # chunks: 1.12912\n",
      "matching chunk: 0 100000\n",
      "matching chunk: 100000 112912\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1465899, 2)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(crime, seg_dc, epsg_dc, close_jn_dist=150, far_jn_dist=150)\n",
    "pts_has_ln.columns = [index_pt, index_seg]\n",
    "pts_has_ln.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "pts_has_ln.shape\n",
    "\n",
    "close_jn_dist=5, far_jn_dist=20: (126773, 2)\n",
    "\n",
    "close_jn_dist=50, far_jn_dist=20: (250848, 2)\n",
    "\n",
    "close_jn_dist=50, far_jn_dist=50: (250848, 2)\n",
    "\n",
    "close_jn_dist=100, far_jn_dist=100 :(758803, 2)\n",
    "\n",
    "close_jn_dist=150, far_jn_dist=150 :(1465899, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean crime data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean crashes\n",
    "date = pd.to_datetime(crime.START_DATE)\n",
    "crime['MONTH'] = date.dt.month\n",
    "crime['YEAR'] = date.dt.year\n",
    "\n",
    "# get dummies for types\n",
    "METHOD_dummies = pd.get_dummies(crime.METHOD, prefix='crime_mtd')\n",
    "METHOD_dummies.replace(0, np.nan, inplace=True)\n",
    "\n",
    "OFFENSE_dummies = pd.get_dummies(crime.OFFENSE, prefix='crime_ofn')\n",
    "OFFENSE_dummies.replace(0, np.nan, inplace=True)\n",
    "\n",
    "crime = crime[['MONTH','YEAR']]\n",
    "crime['crime_total'] = 1\n",
    "crime = crime.merge(METHOD_dummies, left_index=True, right_index=True)\n",
    "crime = crime.merge(OFFENSE_dummies, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_crime = pts_has_ln.merge(crime, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_crime.drop('index_pt', inplace=True, axis=1)\n",
    "\n",
    "ftr_crime = segs_with_crime.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_crime.to_csv(dir_data + fn_feature_crime_dc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
