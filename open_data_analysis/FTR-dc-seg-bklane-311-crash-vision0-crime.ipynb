{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gp\n",
    "from shapely.geometry import Point\n",
    "from src.geom_helper import pts2segs_by_chunk\n",
    "from src.constants import (index_pt, index_seg, epsg_dc, fn_segments_dc, dir_data, \n",
    "                           fn_311_dc, fn_crash_dc, fns_crime_dc, fn_vision0_dc,\n",
    "                          fn_feature_crash_dc, fn_feature_311_dc, fn_feature_vision0_dc, fn_feature_crime_dc,\n",
    "                          fn_oepndc_bk_dc, fn_feature_seg_attr_dc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seg_dc = gp.read_file(dir_data + fn_segments_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# segment attributes features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seg_dc.DIRECTIONALITY = seg_dc.DIRECTIONALITY.apply(lambda x: 'Bi-direction' if x==2 else 'one-way')\n",
    "ftr_segs_col = ['DIRECTIONALITY', 'STREETTYPE', 'SHAPE_Length', 'SEGMENTTYPE']\n",
    "ftr_segs = seg_dc[ftr_segs_col]\n",
    "ftr_segs.index.name = index_seg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ftr_segs.to_csv(dir_data+fn_feature_seg_attr_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bake lanes features in opendc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bk = pd.read_csv('data/raw_data/opendc/Bike_Lane_Street_RightofWay.csv')\n",
    "\n",
    "dum = pd.get_dummies(bk.FACILITY)\n",
    "bk = bk[['STREETSEGID']]\n",
    "bk['bkdc_total'] = 1\n",
    "bk = bk.merge(dum, left_index=True, right_index=True)\n",
    "\n",
    "ftr_bk = bk.groupby('STREETSEGID').sum()\n",
    "ftr_bk.to_csv(dir_data + fn_oepndc_bk_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 311 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (4,15,23,26,32,40) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "csr311 = pd.read_csv(dir_data + fn_311_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match 311 to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 738322 # chunks: 7.38322\n",
      "matching chunk: 0 100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda2\\lib\\site-packages\\numpy\\lib\\function_base.py:2287: RuntimeWarning: invalid value encountered in find_intersects (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matching chunk: 100000 200000\n",
      "matching chunk: 200000 300000\n",
      "matching chunk: 300000 400000\n",
      "matching chunk: 400000 500000\n",
      "matching chunk: 500000 600000\n",
      "matching chunk: 600000 700000\n",
      "matching chunk: 700000 738322\n"
     ]
    }
   ],
   "source": [
    "pts = csr311.apply(lambda x: Point(x.LONGITUDE, x.LATITUDE), axis=1)\n",
    "gpdf = gp.GeoDataFrame(pts, columns=['geometry'])\n",
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(gpdf, seg_dc, epsg_dc)\n",
    "pts_has_ln.columns = [index_pt, index_seg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean 311 data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean csr311\n",
    "date = pd.to_datetime(csr311.INITIATEDDATE)\n",
    "csr311['MONTH'] = date.dt.month\n",
    "csr311['YEAR'] = date.dt.year\n",
    "\n",
    "# get dummies for types\n",
    "dummies = pd.get_dummies(csr311.DESCRIPTION)\n",
    "\n",
    "csr311 = csr311[['MONTH','YEAR']]\n",
    "csr311['311_total'] = 1\n",
    "csr311 = csr311.merge(dummies, left_index=True, right_index=True)\n",
    "csr311['311_total_not_parking_meter'] = csr311['PARKING METER REQUES']==0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_vision0 = pts_has_ln.merge(csr311, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_vision0.drop('index_pt', inplace=True, axis=1)\n",
    "ftr_311 = segs_with_vision0.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "# ftr_311.to_csv(dir_data + fn_feature_311_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features for crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Program Files\\Anaconda2\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2717: DtypeWarning: Columns (4,5,6,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "crashes = pd.read_csv(dir_data+ fn_crash_dc)\n",
    "# clean crashes\n",
    "date = pd.to_datetime(crashes.SOURCEADDTIME)\n",
    "crashes['MONTH'] = date.dt.month\n",
    "crashes['YEAR'] = date.dt.year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match crashes to segments:\n",
    "1. apply pts2seg\n",
    "2. for those(~70K) without coordinates or no matches, use STREETSEGID column in crashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 124278 # chunks: 1.24278\n",
      "matching chunk: 0 100000\n",
      "matching chunk: 100000 124278\n"
     ]
    }
   ],
   "source": [
    "# match with segs\n",
    "no_xy = (crashes.X.isnull())|(crashes.Y.isnull())\n",
    "crashes_with_xy = crashes[~no_xy]\n",
    "pts = crashes_with_xy.apply(lambda x: Point(x.X, x.Y), axis=1)\n",
    "gpdf = gp.GeoDataFrame(pts, columns=['geometry'], index=crashes_with_xy.index)\n",
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(gpdf, seg_dc, epsg_dc)\n",
    "# raname columns\n",
    "pts_has_ln.columns = [index_pt, index_seg]\n",
    "\n",
    "# get STREETSEGID FOR crashes with matches\n",
    "crash_no_ln = crashes[~crashes.index.isin(set(pts_has_ln.index_pt))]\n",
    "crash_using_segid = crash_no_ln.loc[~crash_no_ln.STREETSEGID.isnull(), 'STREETSEGID'].reset_index()\n",
    "# get index_seg\n",
    "crash_using_segid = crash_using_segid.merge(seg_dc[['STREETSEGID']].reset_index(), left_on=\"STREETSEGID\", right_on=\"STREETSEGID\").drop('STREETSEGID', axis=1)\n",
    "# rename columns\n",
    "crash_using_segid.columns=[index_pt, index_seg]\n",
    "\n",
    "# append them\n",
    "pts_segs_index = pts_has_ln.append(crash_using_segid,ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean crashes data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get dummies for types\n",
    "CRASHEVENTTYPES_dummies = crashes.CRASHEVENTTYPES.apply(lambda x: {i:1 for i in x.split(',')}).apply(pd.Series).fillna(0)\n",
    "CRASHEVENTTYPES_dummies.columns = 'CRASHEVENTTYPES_' + CRASHEVENTTYPES_dummies.columns\n",
    "\n",
    "FIRSTHARMFULEVENTSPECIFICS_dummies = pd.get_dummies(crashes.FIRSTHARMFULEVENTSPECIFICS)\n",
    "FIRSTHARMFULEVENTSPECIFICS_dummies.columns = 'FIRSTHARMFULEVENTSPECIFICS_' + FIRSTHARMFULEVENTSPECIFICS_dummies.columns\n",
    "\n",
    "crashes = crashes[['MONTH','YEAR']]\n",
    "crashes['crash_total'] = 1\n",
    "crashes = crashes.merge(CRASHEVENTTYPES_dummies, left_index=True, right_index=True)\n",
    "crashes = crashes.merge(FIRSTHARMFULEVENTSPECIFICS_dummies, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_crashes = pts_segs_index.merge(crashes, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_crashes.drop('index_pt', inplace=True, axis=1)\n",
    "ftr_crashes = segs_with_crashes.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_crashes.to_csv(dir_data + fn_feature_crash_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Vision zero features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vision0 = pd.read_csv(dir_data+ fn_vision0_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match v0 to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pts2segs ing...\n"
     ]
    }
   ],
   "source": [
    "pts = vision0.apply(lambda x: Point(x.X, x.Y), axis=1)\n",
    "gpdf = gp.GeoDataFrame(pts, columns=['geometry'])\n",
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(gpdf, seg_dc, epsg_dc)\n",
    "pts_has_ln.columns = [index_pt, index_seg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean vision zero data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean crashes\n",
    "date = pd.to_datetime(vision0.REQUESTDATE)\n",
    "vision0['MONTH'] = date.dt.month\n",
    "vision0['YEAR'] = date.dt.year\n",
    "\n",
    "# get dummies for types\n",
    "USERTYPE_dummies = pd.get_dummies(vision0.USERTYPE)\n",
    "USERTYPE_dummies.columns = 'USERTYPE_'+USERTYPE_dummies.columns\n",
    "\n",
    "REQUESTTYPE_dummies = pd.get_dummies(vision0.REQUESTTYPE)\n",
    "REQUESTTYPE_dummies.columns = 'REQUESTTYPE_' + REQUESTTYPE_dummies.columns\n",
    "\n",
    "vision0 = vision0[['MONTH','YEAR']]\n",
    "vision0['v0_total'] = 1\n",
    "vision0 = vision0.merge(USERTYPE_dummies, left_index=True, right_index=True)\n",
    "vision0 = vision0.merge(REQUESTTYPE_dummies, left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_vision0 = pts_has_ln.merge(vision0, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_vision0.drop('index_pt', inplace=True, axis=1)\n",
    "ftr_v0 = segs_with_vision0.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_v0.to_csv(dir_data + fn_feature_vision0_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ys = [2014, 2015, 2016, 2017]\n",
    "crimes = [gp.read_file(dir_data + fns_crime_dc[y]) for y in ys]\n",
    "crime = pd.concat(crimes, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### match v0 to segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of data: 112912 # chunks: 1.12912\n",
      "matching chunk: 0 100000\n",
      "matching chunk: 100000 112912\n"
     ]
    }
   ],
   "source": [
    "pts_has_ln, pts_no_ln = pts2segs_by_chunk(crime, seg_dc, epsg_dc)\n",
    "pts_has_ln.columns = [index_pt, index_seg]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean crime data: get YEAR, MONTH and dummy types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# clean crashes\n",
    "date = pd.to_datetime(crime.START_DATE)\n",
    "crime['MONTH'] = date.dt.month\n",
    "crime['YEAR'] = date.dt.year\n",
    "\n",
    "# get dummies for types\n",
    "METHOD_dummies = pd.get_dummies(crime.METHOD)\n",
    "METHOD_dummies.columns = 'METHOD_'+METHOD_dummies.columns\n",
    "\n",
    "OFFENSE_dummies = pd.get_dummies(crime.OFFENSE)\n",
    "OFFENSE_dummies.columns = 'OFFENSE_' + OFFENSE_dummies.columns\n",
    "\n",
    "crime = crime[['MONTH','YEAR']]\n",
    "crime['crime_total'] = 1\n",
    "crime = crime.merge(METHOD_dummies, left_index=True, right_index=True)\n",
    "crime = crime.merge(OFFENSE_dummies, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get features for each seg per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "segs_with_crime = pts_has_ln.merge(crime, left_on=index_pt, right_index=True, how='left')\n",
    "segs_with_crime.drop('index_pt', inplace=True, axis=1)\n",
    "\n",
    "ftr_crime = segs_with_crime.groupby(['index_seg', 'YEAR', 'MONTH']).agg('sum')\n",
    "ftr_crime.to_csv(dir_data + fn_feature_crime_dc)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
